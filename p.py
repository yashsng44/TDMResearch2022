# -*- coding: utf-8 -*-
"""P.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mGVt1XlW_BFao033e0qF-bAgU9QVhcaf
"""

import torch
from torch import nn
from torch.utils.data import DataLoader
from torchvision import datasets
from torchvision.transforms import ToTensor
import torchvision.transforms
from numpy import mean
from numpy import std
from matplotlib import pyplot
import pandas as pd
import math
import numpy as np
import datetime
from torch.utils.data import Dataset

df= pd.read_csv('/content/drive/MyDrive/SUSY.txt', names=['index','phi1','phi2','eta1','eta2','diphi','deta','deltaR','genweight','recoweight'])
susydatanormalized = df.copy()
susydatanormalized.drop(columns='index',inplace=True)
susydatanormalized.drop(columns='genweight',inplace=True)
susydatanormalized.drop(columns='recoweight',inplace=True)
phi1='phi1'
susydatanormalized[phi1] = susydatanormalized[phi1] /math.pi
phi2='phi2'
susydatanormalized[phi2] = susydatanormalized[phi2] /math.pi
eta1='eta1'
susydatanormalized[eta1] = susydatanormalized[eta1] /susydatanormalized[eta1].abs().max()
eta2='eta2'
susydatanormalized[eta2] = susydatanormalized[eta2] /susydatanormalized[eta2].abs().max()
diphi='diphi'
susydatanormalized[diphi] = susydatanormalized[diphi] /susydatanormalized[diphi].abs().max()
deta='deta'
susydatanormalized[deta] = susydatanormalized[deta] /susydatanormalized[deta].abs().max()
deltaR='deltaR'
susydatanormalized[deltaR] = susydatanormalized[deltaR] /susydatanormalized[deltaR].abs().max()
identifier=[1] * 6760
susydatanormalized['identifier']=identifier

df2= pd.read_csv('/content/drive/MyDrive/ttbarsignalplustau_mainSignal.txt', names=['index','phi1','phi2','eta1','eta2','diphi','deta','deltaR','genweight','recoweight'])
standarddatanormalized = df2.copy()
standarddatanormalized=standarddatanormalized[:6760]
standarddatanormalized.drop(columns='index',inplace=True)
standarddatanormalized.drop(columns='genweight',inplace=True)
standarddatanormalized.drop(columns='recoweight',inplace=True)
phi1='phi1'
standarddatanormalized[phi1] = standarddatanormalized[phi1] /math.pi
phi2='phi2'
standarddatanormalized[phi2] = standarddatanormalized[phi2] /math.pi
eta1='eta1'
standarddatanormalized[eta1] = standarddatanormalized[eta1] /standarddatanormalized[eta1].abs().max()
eta2='eta2'
standarddatanormalized[eta2] = standarddatanormalized[eta2] /standarddatanormalized[eta2].abs().max()
diphi='diphi'
standarddatanormalized[diphi] = standarddatanormalized[diphi] /standarddatanormalized[diphi].abs().max()
deta='deta'
standarddatanormalized[deta] = standarddatanormalized[deta] /standarddatanormalized[deta].abs().max()
deltaR='deltaR'
standarddatanormalized[deltaR] = standarddatanormalized[deltaR] /standarddatanormalized[deltaR].abs().max()
identifier=[0] * 6760
standarddatanormalized['identifier']=identifier

"""# New Section"""

frames = [standarddatanormalized,susydatanormalized]
wholedata = pd.concat(frames)
wholedata=wholedata.to_numpy()
np.random.shuffle(wholedata)

trainx=wholedata[0:(round(0.8*len(wholedata[:,2])))]
testx=wholedata[round(0.8*len(wholedata[:,2])):-1]

phi1x,phi2x,eta1x,eta2x,dihpix,detax,deltaRx,identifierx=np.hsplit(trainx,8)
trainX=np.concatenate((phi1x,phi2x,eta1x,eta2x,dihpix,detax,deltaRx),axis=1)

trainY=identifierx
phi1y,phi2y,eta1y,eta2y,dihpiy,detay,deltaRy,identifiery=np.hsplit(testx,8)
testX=np.concatenate((phi1y,phi2y,eta1y,eta2y,dihpiy,detay,deltaRy),axis=1)
testY=identifiery



print('trainx shape',trainX.shape)
print('trainy shape',trainY.shape)
print('testx shape',testX.shape)
print('testy shape',testY.shape)

class CustomDataset(Dataset):
    def __init__(self, classifications, inputsf, transform=None, target_transform=None):
        self.labels = classifications
        self.inputs = inputsf
        self.transform = transform
        self.target_transform = target_transform

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        item = self.inputs[idx]
        label = self.labels[idx]
        return item, label

trainData = CustomDataset(classifications = trainY.astype(np.single), inputsf = trainX.astype(np.single))
testData = CustomDataset(classifications = testY.astype(np.single), inputsf = testX.astype(np.single))

batch_size = 32

# Create data loaders.
train_dataloader = DataLoader(trainData, batch_size=batch_size, shuffle = True)
test_dataloader = DataLoader(testData, batch_size=batch_size, shuffle = True)

for X, y in train_dataloader:
    print(f"Shape of X [N, C, H, W]: {X.shape}")
    print(f"Shape of y: {y.shape} {y.dtype}")
    break

# Get cpu or gpu device for training.
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using {device} device")
#change last node to 1

# Define model
class NeuralNetwork(nn.Module):
    def __init__(self):
        super(NeuralNetwork, self).__init__()
        self.flatten = nn.Flatten()
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(7, 14),
            nn.ReLU(),
            nn.Linear(14, 42),
            nn.ReLU(),
            nn.Linear(42, 21),
            nn.ReLU(),
            nn.Linear(21, 1),
        )

    def forward(self, x):
        x = self.linear_relu_stack(x)
        logits = nn.Sigmoid()
        return logits(x)

model = NeuralNetwork().to(device)
print(model)

loss_fn = nn.BCELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-1)

def train(dataloader, model, loss_fn, optimizer):
    size = len(dataloader.dataset)
    model.train()
    for batch, (X, y) in enumerate(dataloader):
        X, y = X.to(device), y.to(device)

        # Compute prediction error
        pred = model(X)
        loss = loss_fn(pred, y)

        # Backpropagation
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if batch % 100 == 0:
            loss, current = loss.item(), batch * len(X)
            print(f"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]")

def test(dataloader, model, loss_fn):
    size = len(dataloader.dataset)
    num_batches = len(dataloader)
    model.eval()
    test_loss, correct = 0, 0
    with torch.no_grad():
        x=0
        for X, y in dataloader:
            X, y = X.to(device), y.to(device)
            pred = model(X)
            test_loss += loss_fn(pred, y).item()
            #if(x==0):
              #print(pred)
            for i in range(len(pred)):
              for j in range(len(pred[i])):
                if(pred[i][j] < .5):
                  pred[i][j] = 0
                else:
                  pred[i][j] = 1
            #if(x==0):
                #print(pred)
            #x+=1
            correct += (pred == y).type(torch.float).sum().item()
    test_loss /= num_batches
    correct /= size
    print(f"Test Error: \n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \n")

epochs = 1000

for t in range(epochs):
    print(f"Epoch {t+1}\n-------------------------------")
    train(train_dataloader, model, loss_fn, optimizer)
    test(test_dataloader, model, loss_fn)
    print("Done!")

optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum =.5)

optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum =.5)

batch_size = 16

# Create data loaders.
train_dataloader = DataLoader(trainData, batch_size=batch_size, shuffle = True)
test_dataloader = DataLoader(testData, batch_size=batch_size, shuffle = True)

epochs = 300

for t in range(epochs):
    print(f"Epoch {t+1}\n-------------------------------")
    train(train_dataloader, model, loss_fn, optimizer)
    test(test_dataloader, model, loss_fn)
    fullTest(model)
    print("Done!")

test_loss = 0
pred = model(torch.from_numpy(testX.astype(np.single)))
test_loss += loss_fn(pred, torch.from_numpy(testYY.astype(np.single))).item()
print(test_loss)
print(testX)
print(pred)
print(testYY)
test_loss = 0
pred = model(torch.from_numpy(trainX.astype(np.single)))
test_loss += loss_fn(pred, torch.from_numpy(trainYY.astype(np.single))).item()
print(test_loss)

test_loss = 0
pred = model(torch.from_numpy(testX.astype(np.single)))
test_loss += loss_fn(pred, torch.from_numpy(testYY.astype(np.single))).item()
print(test_loss)
print(testX)
print(torch.round(input = pred, decimals = 3))
print(testYY)
test_loss = 0
pred = model(torch.from_numpy(trainX.astype(np.single)))
test_loss += loss_fn(pred, torch.from_numpy(trainYY.astype(np.single))).item()
print(test_loss)
fullTest(model)

torch.save(model.state_dict(), '/content/drive/MyDrive/myModel')